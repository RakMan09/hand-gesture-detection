# Code Structure Explanation

This section provides a detailed breakdown of the project's codebase organization, explaining the purpose and relationships between different components.

## ğŸ“‚ Overall Project Organization

```
hand-gesture-detection/
â”‚
â”œâ”€â”€ android/                          # Android Studio Project
â”‚   â””â”€â”€ app/src/main/
â”‚       â”œâ”€â”€ java/com/hci/gesturetouchless/
â”‚       â”œâ”€â”€ res/                      # UI resources
â”‚       â””â”€â”€ assets/                   # Model files
â”‚
â””â”€â”€ ml-training/                      # Python ML Pipeline
    â”œâ”€â”€ notebooks/train.ipynb         # Training code
    â”œâ”€â”€ src/                          # Python modules
    â”œâ”€â”€ data/                         # Datasets
    â””â”€â”€ models/                       # Trained models
```

---

## ğŸ—ï¸ Android Application Architecture

### Layer Structure

The Android app follows a **layered architecture** with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Presentation Layer              â”‚
â”‚  (UI: Activities, Fragments)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Business Logic Layer            â”‚
â”‚  (Services, ViewModels, Controllers)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Model/Data Layer                â”‚
â”‚  (Models, Data Classes, Preferences)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         ML Layer                        â”‚
â”‚  (Gesture Classifier, Landmark Utils)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Native/System Layer             â”‚
â”‚  (Camera, MediaPipe, Android APIs)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Package Structure

```
com.hci.gesturetouchless/
â”‚
â”œâ”€â”€ MainActivity.kt                       # Entry point, foreground detection
â”‚
â”œâ”€â”€ SettingsActivity.kt                   # Settings and configuration
â”‚
â”œâ”€â”€ models/                               # Data classes
â”‚   â”œâ”€â”€ GestureAction.kt                  # Gesture actions enum
â”‚   â”œâ”€â”€ GestureType.kt                    # Supported gesture types
â”‚   â””â”€â”€ GestureMapping.kt                 # Gesture-to-action mappings
â”‚
â”œâ”€â”€ services/                             # Background services
â”‚   â”œâ”€â”€ GestureDetectionService.kt        # Background detection
â”‚   â””â”€â”€ GestureAccessibilityService.kt    # System-level integration
â”‚
â”œâ”€â”€ ml/                                   # Machine learning
â”‚   â””â”€â”€ GestureClassifier.kt              # TensorFlow Lite wrapper
â”‚
â””â”€â”€ utils/                                # Helper utilities
    â”œâ”€â”€ LandmarkUtils.kt                  # Hand landmark processing
    â””â”€â”€ PreferencesManager.kt             # Shared preferences
```

---

## ğŸ“‹ Component Details

### 1. **Presentation Layer** - User Interface

#### MainActivity.kt
**Purpose:** Main application screen with real-time gesture detection
**Key Responsibilities:**
- Display camera preview using CameraX
- Show detected gestures in real-time
- Handle permission requests
- Manage app lifecycle (foreground detection)

**Key Methods:**
```kotlin
onCreate()           â†’ Initialize app, load ML models
onStart()            â†’ Start camera for detection
onStop()             â†’ Stop camera, save battery
startCamera()        â†’ Bind CameraX use cases
bindCameraUseCases() â†’ Connect preview and image analysis
```

**Data Flow:**
```
Camera Frame â†’ CameraX â†’ Image Processor â†’ Landmarks â†’ Classification â†’ UI Update
```

#### SettingsActivity.kt
**Purpose:** Allow users to configure gesture recognition
**Features:**
- Adjust confidence threshold
- Enable/disable specific gestures
- Map gestures to actions
- Toggle accessibility service

---

### 2. **Service Layer** - Background & System Integration

#### GestureDetectionService.kt
**Purpose:** Run gesture detection in background even when app is minimized
**Type:** Foreground Service (required on Android 8+)

**Lifecycle:**
```
onStartCommand() â†’ Initialize camera & models
â†“
startDetection() â†’ Continuous gesture monitoring
â†“
onDestroy() â†’ Stop camera, clean resources
```

**Key Features:**
- Persistent notification showing service is running
- Auto-restart if killed (START_STICKY)
- Can run alongside foreground app
- Communicates gestures to accessibility service

#### GestureAccessibilityService.kt
**Purpose:** Execute gesture-triggered actions at system level
**Type:** AccessibilityService (requires explicit user permission)

**Integrations:**
- Receives gesture events from detection service
- Executes system actions (volume, screenshot, etc.)
- Handles accessibility event processing
- Integrates with gesture mapping configuration

**Event Flow:**
```
Gesture Detected
    â†“
GestureDetectionService broadcasts
    â†“
GestureAccessibilityService receives
    â†“
Look up action in GestureMapping
    â†“
Execute system action (VOLUME_UP, etc.)
```

---

### 3. **Model Layer** - Data Structures

#### GestureType.kt
**Purpose:** Define supported gesture types
**Structure:**
```kotlin
enum class GestureType {
    THUMBS_UP,      // Thumb pointing up
    PEACE_SIGN,     // Two fingers extended
    FIST,           // Closed hand
    OPEN_PALM,      // All fingers extended
    POINTING        // Index finger extended
}
```
**Usage:** Type-safe gesture reference throughout app

#### GestureAction.kt
**Purpose:** Define possible gesture actions
**Examples:**
```kotlin
enum class GestureAction {
    VOLUME_UP,
    VOLUME_DOWN,
    TAKE_SCREENSHOT,
    OPEN_RECENT_APPS,
    LOCK_SCREEN
}
```

#### GestureMapping.kt
**Purpose:** Map gestures to actions
**Data Structure:**
```kotlin
data class GestureMapping(
    val gesture: String,          // "thumbs_up", "peace_sign", etc.
    val action: GestureAction,    // What to do when gesture detected
    val enabled: Boolean = true   // Is this mapping active?
)
```
**Storage:** Persisted in SharedPreferences via PreferencesManager

---

### 4. **ML Layer** - Core Recognition Engine

#### GestureClassifier.kt
**Purpose:** Wrap TensorFlow Lite model for inference

**Architecture:**
```
Hand Landmarks (63 features)
    â†“
Normalize (using mean/std)
    â†“
TensorFlow Lite Interpreter
    â†“
Dense Layer 1 (256 units, ReLU)
    â†“
Dense Layer 2 (128 units, ReLU)
    â†“
Dense Layer 3 (64 units, ReLU)
    â†“
Output Layer (5 units, Softmax)
    â†“
Probabilities for 5 gestures
```

**Key Methods:**
```kotlin
classify()              â†’ Raw classification (single prediction)
classifyWithSmoothing() â†’ Temporal smoothing (more stable)
resetHistory()          â†’ Clear prediction history
```

**Features:**
- Lazy initialization (models load only when needed)
- Temporal smoothing to reduce false positives
- Configurable confidence threshold
- Automatic normalization using stored mean/std

#### LandmarkUtils.kt
**Purpose:** Process hand landmarks from MediaPipe

**Key Operations:**
```kotlin
normalizeLandmarks()    â†’ Convert 21 landmarks to 63-element feature vector
smoothLandmarks()       â†’ Reduce jitter using Kalman filtering
calculateHandSpread()   â†’ Compute gesture-specific metrics
isValidLandmarks()      â†’ Quality validation
```

**Input:** MediaPipe hand landmark points (21 points Ã— 3 coordinates)
**Output:** Normalized feature vector ready for ML model

---

### 5. **Utility Layer** - Helper Functions

#### PreferencesManager.kt
**Purpose:** Persist user settings using SharedPreferences

**Stored Data:**
```kotlin
- confidenceThreshold    â†’ Min confidence to trigger action (default 0.7)
- enabledGestures        â†’ Which gestures to recognize
- gestureActions         â†’ Custom gesture-to-action mappings
- accessibilityEnabled   â†’ Is background service enabled
```

**Pattern:** Singleton manager for centralized preference access

---

## ğŸ”„ Data Flow Example: Gesture Detection

### 1. Frame Acquisition
```
Device Camera
    â†“
CameraX ImageAnalysis Use Case
    â†“
Image Frame (RGB bitmap)
```

### 2. Hand Detection
```
Image Frame
    â†“
MediaPipe HandLandmarker
    â†“
21 Hand Landmarks (x, y, z normalized [0,1])
```

### 3. Feature Extraction
```
21 Landmarks
    â†“
LandmarkUtils.normalizeLandmarks()
    â†“
63-element Float Array
    â†“
LandmarkUtils.smoothLandmarks()
    â†“
Smoothed 63-element Float Array
```

### 4. Classification
```
Smoothed Features
    â†“
GestureClassifier.classifyWithSmoothing()
    â†“
[0.92, 0.03, 0.02, 0.02, 0.01] â† Probabilities for 5 gestures
    â†“
Apply Confidence Threshold (0.7)
    â†“
Gesture Name + Confidence: ("thumbs_up", 0.92)
```

### 5. Action Execution
```
Gesture Result
    â†“
PreferencesManager.getGestureAction()
    â†“
GestureAction.VOLUME_UP
    â†“
GestureAccessibilityService.performAction()
    â†“
System Action Executed
```

---

## ğŸ“¦ Dependencies & Their Roles

### Android Framework
- **CameraX** (camera input) - Modern camera API with lifecycle awareness
- **AndroidX** (core libraries) - Compatibility and modern Android components
- **MediaPipe** (landmark detection) - Google's hand detection model

### ML/TensorFlow
- **TensorFlow Lite** (model inference) - Lightweight ML inference on device
- **Gson** (JSON parsing) - Load gesture labels and normalization parameters

### Data Storage
- **SharedPreferences** - User settings and gesture mappings
- **DataStore** - Modern alternative to SharedPreferences

---

## ğŸ”Œ Integration Points

### Camera to ML Pipeline
```
CameraX
    â†“
ImageAnalysis Analyzer
    â†“
Process Frame
    â†“
Extract MediaPipe Landmarks
    â†“
Classify Gesture
    â†“
Update UI / Trigger Action
```

### Settings to Detection
```
User Changes Settings
    â†“
PreferencesManager.save()
    â†“
Settings persisted
    â†“
Detection service reads on next frame
    â†“
Applied immediately
```

### Services Communication
```
MainActivity (Foreground)
    â†“
Stops camera when minimized
    â†“
Signals GestureDetectionService
    â†“
Service takes over camera access
    â†“
GestureAccessibilityService notified
    â†“
Ready to execute actions
```

---

## ğŸ§µ Threading Model

### Main Thread
- UI updates (gesture text, status display)
- Activity lifecycle methods
- User input handling

### Background Threads
```
â”œâ”€â”€ Camera Thread (CameraX)
â”‚   â””â”€â”€ Processes video frames continuously
â”‚
â”œâ”€â”€ ML Thread (Gesture Classification)
â”‚   â””â”€â”€ Runs TensorFlow Lite inference
â”‚
â””â”€â”€ Executor Thread (GestureClassifier)
    â””â”€â”€ Lazy initializes ML models
```

**Thread Safety:**
- Use `@Volatile` for shared variables between threads
- Use `synchronized` blocks for critical sections
- Use Coroutines/RxJava for async operations

---

## ğŸ’¾ Memory Management

### Model Loading
```
GestureClassifier.ensureInitialized()
    â†“
Load TensorFlow Lite model from assets
    â†“
Keep in memory (reused for multiple predictions)
    â†“
Close on app shutdown
```

### Frame Processing
```
Process current frame
    â†“
Extract landmarks
    â†“
Discard frame (GC)
    â†“
Process next frame
```

### Optimization
- Models loaded once and reused
- Temporal smoothing uses fixed-size buffer
- Images discarded after processing
- Resources cleaned in `onDestroy()`

---

## ğŸ” Permission Handling

```
Runtime Permissions (Android 6+)
    â”œâ”€â”€ CAMERA (required for video input)
    â””â”€â”€ POST_NOTIFICATIONS (required for foreground service)

Accessibility Service
    â””â”€â”€ User explicitly enables in Settings
        â†’ GestureAccessibilityService gains system access

Manifest Permissions
    â”œâ”€â”€ FOREGROUND_SERVICE (background detection)
    â”œâ”€â”€ FOREGROUND_SERVICE_CAMERA
    â””â”€â”€ MODIFY_AUDIO_SETTINGS (volume control)
```

---

## ğŸ“Š Configuration & State

### App State
```
Stored in:
â”œâ”€â”€ SharedPreferences (via PreferencesManager)
â”‚   â”œâ”€â”€ Confidence threshold
â”‚   â”œâ”€â”€ Enabled gestures
â”‚   â””â”€â”€ Gesture-action mappings
â”‚
â”œâ”€â”€ In-Memory (Instance Variables)
â”‚   â”œâ”€â”€ Camera provider
â”‚   â”œâ”€â”€ ML models
â”‚   â””â”€â”€ Recent predictions
â”‚
â””â”€â”€ Model Assets
    â”œâ”€â”€ hand_model.tflite (ML classifier)
    â”œâ”€â”€ hand_landmarker.task (MediaPipe model)
    â””â”€â”€ hand_labels.json (gesture labels + normalization)
```

### Lifecycle Transitions
```
APP LAUNCHED
    â†“
MainActivity.onCreate() â†’ Load models
    â†“
MainActivity.onStart() â†’ Start foreground camera
    â†“
APP IN FOREGROUND
    â””â”€ Real-time detection in UI
    â†“
USER MINIMIZES APP
    â†“
MainActivity.onStop() â†’ Stop camera
    â†“
GestureDetectionService.onStart() â†’ Take over camera
    â†“
APP IN BACKGROUND
    â””â”€ Background detection continues
    â†“
USER RETURNS TO APP
    â†“
MainActivity.onStart() â†’ Resume foreground camera
    â†“
GestureDetectionService.onDestroy() â†’ Release background service
    â†“
APP CLOSED
    â””â”€ Clean up resources
```

---

## ğŸ” Key Design Patterns

### 1. **Singleton Pattern**
- PreferencesManager - Single instance manages all preferences
- GestureClassifier - Single model instance reused across app

### 2. **Service Pattern**
- GestureDetectionService - Background processing
- GestureAccessibilityService - System integration

### 3. **Observer Pattern**
- Camera frames trigger callbacks
- Gestures notify listeners
- Accessibility events trigger actions

### 4. **Strategy Pattern**
- Different detection strategies (raw vs. smoothed classification)
- Configurable gesture-to-action mapping

### 5. **Lazy Initialization**
- ML models load only when needed
- Reduces startup time and memory usage

---

## ğŸš€ Performance Considerations

### Optimization Techniques
1. **Frame Skipping** - Process every 2nd/3rd frame if needed
2. **Model Quantization** - Smaller model size, faster inference
3. **Temporal Smoothing** - Reduce false positives
4. **Thread Pooling** - Reuse threads, reduce overhead

### Bottlenecks
```
Camera Frame Acquisition    ~5-8ms
MediaPipe Landmark Detection ~15-25ms
Feature Extraction          ~1-2ms
TensorFlow Inference        ~5-8ms
Post-processing             ~1-2ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total                       ~27-45ms per frame
Effective FPS               23-37 FPS
```

---

## ğŸ§ª Testing Strategy

### Unit Testing
```
GestureClassifier
    â”œâ”€â”€ Test normalization
    â”œâ”€â”€ Test classification
    â””â”€â”€ Test smoothing

LandmarkUtils
    â”œâ”€â”€ Test landmark processing
    â””â”€â”€ Test validation
```

### Integration Testing
```
MainActivity
    â”œâ”€â”€ Test camera binding
    â”œâ”€â”€ Test gesture detection
    â””â”€â”€ Test settings integration

Services
    â”œâ”€â”€ Test background detection
    â””â”€â”€ Test action execution
```

### Manual Testing
```
Device Testing
    â”œâ”€â”€ Real gesture recognition
    â”œâ”€â”€ Background service
    â””â”€â”€ Accessibility integration
```

---

## ğŸ“ˆ Scalability & Extension

### Adding New Gestures
1. Collect training data for new gesture
2. Retrain model in ml-training/
3. Export new TFLite model
4. Update GestureType enum
5. Add new gesture mapping in settings

### Adding New Actions
1. Add to GestureAction enum
2. Implement in GestureAccessibilityService
3. Expose in settings UI
4. Test with various gestures

### Performance Improvements
1. Quantize model to INT8 format
2. Implement frame skipping
3. Use GPU acceleration if available
4. Optimize landmark smoothing

---
