{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 1A: Importing ans Installing modules\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "\n",
        "!pip install -q \"mediapipe==0.10.14\" \"opencv-python-headless==4.8.1.78\" pandas matplotlib tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANhBghYjqLxh",
        "outputId": "20c47424-85d7-47e2-eaeb-cbc7284187c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "NumPy: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 1B: Initialize TPU/GPU Strategy\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Try TPU, fallback to GPU/CPU\n",
        "USE_TPU = False\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    USE_TPU = True\n",
        "    print(f\"‚úÖ Running on TPU: {tpu.cluster_spec().as_dict()['worker']}\")\n",
        "except (ValueError, tf.errors.NotFoundError):\n",
        "    # No TPU found, use GPU if available\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "        print(f\"‚úÖ Running on {len(gpus)} GPU(s)\")\n",
        "    else:\n",
        "        strategy = tf.distribute.get_strategy()  # Default (single CPU/GPU)\n",
        "        print(\"‚úÖ Running on CPU/single GPU\")\n",
        "\n",
        "print(f\"Strategy: {strategy.__class__.__name__}\")\n",
        "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LonrfxUH3Z6N",
        "outputId": "089c7f6d-0344-42f4-dfb1-cb2abf1313fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Running on 1 GPU(s)\n",
            "Strategy: MirroredStrategy\n",
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 2: Mount Drive & Set Project Directory\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training\"\n",
        "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "\n",
        "# Do NOT re-install mediapipe here; it is already installed in Cell 1.\n",
        "# If you really need extra libs later, install them separately.\n",
        "import mediapipe as mp\n",
        "print(\"MediaPipe version:\", mp.__version__)"
      ],
      "metadata": {
        "id": "osfI4B-42tPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea804a0e-4c3d-4fd8-993d-9c5fa62df340"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Current directory: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training\n",
            "MediaPipe version: 0.10.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 3: Define Target Gestures\n",
        "# ============================================================\n",
        "\n",
        "# Hand gestures from HaGRID (model labels)\n",
        "# Mapping comments show how you will use them in the app.\n",
        "HAND_GESTURES = {\n",
        "    \"call\":    \"call\",    # VOICE_ASSISTANT\n",
        "    \"like\":    \"like\",    # INCREASE_VOLUME\n",
        "    \"dislike\": \"dislike\", # DECREASE_VOLUME\n",
        "    \"fist\":    \"fist\",    # PLAY_PAUSE\n",
        "    \"palm\":    \"palm\",    # neutral / no-op\n",
        "    \"peace\":   \"peace\",   # OPEN_CAMERA\n",
        "    \"rock\":    \"rock\",    # SCREENSHOT\n",
        "}\n",
        "\n",
        "print(\"Hand gestures to train:\", list(HAND_GESTURES.keys()))\n",
        "\n",
        "ALLOWED_HAND_GESTURES = set(HAND_GESTURES.keys())"
      ],
      "metadata": {
        "id": "dcLspZ732vKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2016b5ca-7fb2-4375-d719-49ac642850cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hand gestures to train: ['call', 'like', 'dislike', 'fist', 'palm', 'peace', 'rock']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 4: Dataset Configuration Flags\n",
        "# ============================================================\n",
        "\n",
        "# Control which datasets to use for training\n",
        "USE_PUBLIC_DATA = True   # Include HaGRID public dataset\n",
        "USE_MANUAL_DATA = True   # Include manually collected data\n",
        "\n",
        "print(\"Dataset Configuration:\")\n",
        "print(f\"  ‚Ä¢ Use HaGRID Public Data: {USE_PUBLIC_DATA}\")\n",
        "print(f\"  ‚Ä¢ Use Manual Collected Data: {USE_MANUAL_DATA}\")\n",
        "\n",
        "if not USE_PUBLIC_DATA and not USE_MANUAL_DATA:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Both datasets disabled! Must enable at least one.\")"
      ],
      "metadata": {
        "id": "HkVDP6oHJxce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25743a84-fd01-4d3c-94e5-5f6e8eb61ed2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Configuration:\n",
            "  ‚Ä¢ Use HaGRID Public Data: True\n",
            "  ‚Ä¢ Use Manual Collected Data: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 5: Extract HaGRID subset (Optimized + Stratified)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Configuration\n",
        "HAGRID_ZIP_PATH = \"/content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/datasets/hagrid-classification-512p.zip\"\n",
        "HAGRID_SUBSET_DIR = os.path.join(PROJECT_ROOT, \"hagrid_subset\")\n",
        "\n",
        "# NEW: Optimal sample size with stratified sampling\n",
        "SAMPLES_PER_GESTURE = 1500  # ‚Üê Increased from 500\n",
        "USE_STRATIFIED_SAMPLING = True  # ‚Üê Ensures diversity\n",
        "\n",
        "# Skip if not using public data\n",
        "if not USE_PUBLIC_DATA:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚è≠Ô∏è  SKIPPING HAGRID EXTRACTION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"USE_PUBLIC_DATA = False, skipping HaGRID dataset extraction\")\n",
        "    print(\"\\n‚úì Moving to next step...\")\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"EXTRACTING HAGRID SUBSET (OPTIMIZED)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Samples per gesture: {SAMPLES_PER_GESTURE}\")\n",
        "    print(f\"Stratified sampling: {USE_STRATIFIED_SAMPLING}\")\n",
        "\n",
        "    # Check if already extracted\n",
        "    already_extracted = True\n",
        "    if os.path.exists(HAGRID_SUBSET_DIR):\n",
        "        for gesture_name in HAND_GESTURES.keys():\n",
        "            gesture_dir = os.path.join(HAGRID_SUBSET_DIR, gesture_name)\n",
        "            if not os.path.exists(gesture_dir):\n",
        "                already_extracted = False\n",
        "                break\n",
        "            existing_count = len([f for f in os.listdir(gesture_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "            if existing_count < SAMPLES_PER_GESTURE:\n",
        "                already_extracted = False\n",
        "                break\n",
        "    else:\n",
        "        already_extracted = False\n",
        "\n",
        "    if already_extracted:\n",
        "        print(f\"\\n‚úì HaGRID subset already extracted to {HAGRID_SUBSET_DIR}\")\n",
        "        print(\"Skipping extraction...\")\n",
        "    else:\n",
        "        # Check if ZIP exists\n",
        "        if not os.path.exists(HAGRID_ZIP_PATH):\n",
        "            print(\"‚ö†Ô∏è HaGRID zip not found at:\", HAGRID_ZIP_PATH)\n",
        "            print(\"\\nOptions:\")\n",
        "            print(\"1. Set USE_PUBLIC_DATA = False to skip HaGRID\")\n",
        "            print(\"2. Download HaGRID and place at:\", HAGRID_ZIP_PATH)\n",
        "        else:\n",
        "            os.makedirs(HAGRID_SUBSET_DIR, exist_ok=True)\n",
        "\n",
        "            # Function to extract with retry\n",
        "            def extract_with_retry(zf, member, dest_path, max_retries=3):\n",
        "                \"\"\"Extract a single file with retry logic for Drive disconnections.\"\"\"\n",
        "                for attempt in range(max_retries):\n",
        "                    try:\n",
        "                        with zf.open(member) as src, open(dest_path, \"wb\") as dst:\n",
        "                            dst.write(src.read())\n",
        "                        return True\n",
        "                    except OSError as e:\n",
        "                        if \"Transport endpoint\" in str(e) or \"errno 107\" in str(e).lower():\n",
        "                            if attempt < max_retries - 1:\n",
        "                                print(f\"\\n‚ö†Ô∏è Connection lost, retrying ({attempt + 1}/{max_retries})...\")\n",
        "                                time.sleep(2)\n",
        "                                continue\n",
        "                            else:\n",
        "                                print(f\"\\n‚ùå Failed after {max_retries} attempts: {member.filename}\")\n",
        "                                return False\n",
        "                        else:\n",
        "                            raise\n",
        "                return False\n",
        "\n",
        "            print(f\"\\nExtracting {SAMPLES_PER_GESTURE} samples per gesture from ZIP...\")\n",
        "\n",
        "            try:\n",
        "                with zipfile.ZipFile(HAGRID_ZIP_PATH, \"r\") as zf:\n",
        "                    all_names = zf.namelist()\n",
        "                    print(\"Total entries in zip:\", len(all_names))\n",
        "\n",
        "                    for gesture_name in HAND_GESTURES.keys():\n",
        "                        gesture_dir = os.path.join(HAGRID_SUBSET_DIR, gesture_name)\n",
        "                        os.makedirs(gesture_dir, exist_ok=True)\n",
        "\n",
        "                        # Find images for this gesture\n",
        "                        pattern = f\"hagrid-classification-512p/{gesture_name}/\"\n",
        "                        candidates = [\n",
        "                            name for name in all_names\n",
        "                            if pattern in name and name.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
        "                        ]\n",
        "\n",
        "                        if not candidates:\n",
        "                            print(f\"\\n‚ö†Ô∏è No images found for gesture: {gesture_name}\")\n",
        "                            continue\n",
        "\n",
        "                        print(f\"\\n{gesture_name}: found {len(candidates)} images under '{pattern}'\")\n",
        "\n",
        "                        # NEW: Stratified sampling for better diversity\n",
        "                        if USE_STRATIFIED_SAMPLING and len(candidates) > SAMPLES_PER_GESTURE:\n",
        "                            # Shuffle with fixed seed for reproducibility\n",
        "                            random.Random(42).shuffle(candidates)\n",
        "\n",
        "                            # Take evenly spaced samples across the dataset\n",
        "                            step = len(candidates) / SAMPLES_PER_GESTURE\n",
        "                            selected = [candidates[int(i * step)] for i in range(SAMPLES_PER_GESTURE)]\n",
        "\n",
        "                            print(f\"  Using stratified sampling (step={step:.1f})\")\n",
        "                        else:\n",
        "                            # Fallback to first N samples\n",
        "                            selected = candidates[:SAMPLES_PER_GESTURE]\n",
        "\n",
        "                        successful = 0\n",
        "                        failed = 0\n",
        "\n",
        "                        for img_name in tqdm(selected, desc=f\"Extracting {gesture_name}\"):\n",
        "                            base_name = os.path.basename(img_name)\n",
        "                            dest_path = os.path.join(gesture_dir, base_name)\n",
        "\n",
        "                            # Skip if already extracted\n",
        "                            if os.path.exists(dest_path):\n",
        "                                successful += 1\n",
        "                                continue\n",
        "\n",
        "                            # Extract with retry\n",
        "                            if extract_with_retry(zf, img_name, dest_path):\n",
        "                                successful += 1\n",
        "                            else:\n",
        "                                failed += 1\n",
        "\n",
        "                        print(f\"  ‚úì {successful} successful, {failed} failed\")\n",
        "\n",
        "                print(\"\\n‚úì HaGRID subset extraction complete!\")\n",
        "\n",
        "            except OSError as e:\n",
        "                if \"Transport endpoint\" in str(e) or \"errno 107\" in str(e).lower():\n",
        "                    print(\"\\n\" + \"=\" * 60)\n",
        "                    print(\"‚ùå GOOGLE DRIVE CONNECTION ERROR\")\n",
        "                    print(\"=\" * 60)\n",
        "                    print(\"\\nThe connection to Google Drive was lost during extraction.\")\n",
        "                    print(\"\\nüîß SOLUTIONS:\")\n",
        "                    print(\"\\n1. COPY ZIP TO COLAB DISK (RECOMMENDED):\")\n",
        "                    print(\"   Run this in a new cell:\")\n",
        "                    print(\"   ```\")\n",
        "                    print(\"   import shutil\")\n",
        "                    print(f\"   shutil.copy('{HAGRID_ZIP_PATH}', '/tmp/hagrid.zip')\")\n",
        "                    print(\"   HAGRID_ZIP_PATH = '/tmp/hagrid.zip'\")\n",
        "                    print(\"   ```\")\n",
        "                    print(\"   Then re-run this cell.\")\n",
        "                    print(\"\\n2. OR SET USE_PUBLIC_DATA = False:\")\n",
        "                    print(\"   Use only manual data for training\")\n",
        "                    raise\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "_o9mo3Po2x5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084d1453-b924-4fb3-ee21-d458dd5eec4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EXTRACTING HAGRID SUBSET (OPTIMIZED)\n",
            "============================================================\n",
            "Samples per gesture: 1500\n",
            "Stratified sampling: True\n",
            "\n",
            "‚úì HaGRID subset already extracted to /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/hagrid_subset\n",
            "Skipping extraction...\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6: Process Manual Images from Folder Structure\n",
        "# ============================================================\n",
        "\n",
        "import mediapipe as mp\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from PIL import Image as PILImage\n",
        "import numpy as np\n",
        "\n",
        "# Manual data configuration\n",
        "MANUAL_IMAGES_DIR = os.path.join(PROJECT_ROOT, \"data/hand_gestures_manual\")\n",
        "MANUAL_JSON_DIR = os.path.join(PROJECT_ROOT, \"data/hand_gestures_manual_json\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîÑ PROCESSING MANUAL IMAGES FROM FOLDERS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nInput: {MANUAL_IMAGES_DIR}\")\n",
        "print(f\"Output: {MANUAL_JSON_DIR}\")\n",
        "print(f\"\\nExpected folder structure:\")\n",
        "print(f\"  {MANUAL_IMAGES_DIR}/\")\n",
        "print(f\"    ‚îú‚îÄ‚îÄ call/       (image files)\")\n",
        "print(f\"    ‚îú‚îÄ‚îÄ like/       (image files)\")\n",
        "print(f\"    ‚îú‚îÄ‚îÄ dislike/    (image files)\")\n",
        "print(f\"    ‚îú‚îÄ‚îÄ fist/       (image files)\")\n",
        "print(f\"    ‚îú‚îÄ‚îÄ palm/       (image files)\")\n",
        "print(f\"    ‚îú‚îÄ‚îÄ peace/      (image files)\")\n",
        "print(f\"    ‚îî‚îÄ‚îÄ rock/       (image files)\")\n",
        "\n",
        "\n",
        "def process_manual_images():\n",
        "    \"\"\"\n",
        "    Process all images from gesture folders and extract MediaPipe landmarks.\n",
        "\n",
        "    Folder structure:\n",
        "        data/hand_gestures_manual/\n",
        "            call/\n",
        "                img1.jpg\n",
        "                img2.jpg\n",
        "                ...\n",
        "            like/\n",
        "                img1.jpg\n",
        "                ...\n",
        "\n",
        "    Output:\n",
        "        data/hand_gestures_manual_json/\n",
        "            call_manual_001.json\n",
        "            call_manual_002.json\n",
        "            ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(MANUAL_JSON_DIR, exist_ok=True)\n",
        "\n",
        "    # Initialize MediaPipe Hands\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=1,\n",
        "        min_detection_confidence=0.5,\n",
        "    )\n",
        "\n",
        "    # Supported image extensions\n",
        "    IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "    # Check if manual images directory exists\n",
        "    if not os.path.exists(MANUAL_IMAGES_DIR):\n",
        "        print(f\"\\n‚ö†Ô∏è Manual images directory not found: {MANUAL_IMAGES_DIR}\")\n",
        "        print(\"\\nPlease create the folder structure and add your images:\")\n",
        "        print(f\"  1. Create folder: {MANUAL_IMAGES_DIR}\")\n",
        "        print(\"  2. Create subfolders for each gesture (call, like, dislike, etc.)\")\n",
        "        print(\"  3. Add image files to each gesture folder\")\n",
        "        print(\"  4. Run this cell again\")\n",
        "        return\n",
        "\n",
        "    # Find all gesture folders\n",
        "    gesture_folders = [\n",
        "        d\n",
        "        for d in os.listdir(MANUAL_IMAGES_DIR)\n",
        "        if os.path.isdir(os.path.join(MANUAL_IMAGES_DIR, d))\n",
        "    ]\n",
        "\n",
        "    if not gesture_folders:\n",
        "        print(f\"\\n‚ö†Ô∏è No gesture folders found in {MANUAL_IMAGES_DIR}\")\n",
        "        print(\"\\nCreate subfolders like: call/, like/, dislike/, etc.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüìÅ Found {len(gesture_folders)} gesture folders: {gesture_folders}\")\n",
        "\n",
        "    total_stats = {\n",
        "        \"total_images\": 0,\n",
        "        \"successful\": 0,\n",
        "        \"no_hand\": 0,\n",
        "        \"error\": 0,\n",
        "    }\n",
        "\n",
        "    gesture_stats = {}\n",
        "\n",
        "    # Process each gesture folder\n",
        "    for gesture_name in gesture_folders:\n",
        "        gesture_path = os.path.join(MANUAL_IMAGES_DIR, gesture_name)\n",
        "\n",
        "        # Find all image files in this folder\n",
        "        image_files = [\n",
        "            f\n",
        "            for f in os.listdir(gesture_path)\n",
        "            if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS\n",
        "        ]\n",
        "\n",
        "        if not image_files:\n",
        "            print(f\"\\n‚ö†Ô∏è No images found in {gesture_name}/ folder\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"üì∏ Processing: {gesture_name.upper()}\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(f\"Found {len(image_files)} images\")\n",
        "\n",
        "        successful = 0\n",
        "        no_hand = 0\n",
        "        error = 0\n",
        "\n",
        "        # Process each image\n",
        "        for img_file in tqdm(image_files, desc=f\"  Processing {gesture_name}\"):\n",
        "            img_path = os.path.join(gesture_path, img_file)\n",
        "\n",
        "            try:\n",
        "                # Load image\n",
        "                image = PILImage.open(img_path)\n",
        "                img_np = np.array(image.convert(\"RGB\"))\n",
        "\n",
        "                # Process with MediaPipe\n",
        "                results = hands.process(img_np)\n",
        "\n",
        "                if not results.multi_hand_landmarks:\n",
        "                    no_hand += 1\n",
        "                    continue\n",
        "\n",
        "                # Extract landmarks\n",
        "                lm = results.multi_hand_landmarks[0]\n",
        "                flat = []\n",
        "                for p in lm.landmark:\n",
        "                    flat.extend([p.x, p.y, p.z])\n",
        "\n",
        "                # Generate unique filename\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
        "                json_filename = f\"{gesture_name}_manual_{timestamp}.json\"\n",
        "                json_path = os.path.join(MANUAL_JSON_DIR, json_filename)\n",
        "\n",
        "                # Save to JSON\n",
        "                json_data = {\n",
        "                    \"gesture\": gesture_name,\n",
        "                    \"landmarks\": flat,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"source\": \"manual_folder\",\n",
        "                    \"original_file\": img_file,\n",
        "                    \"original_path\": img_path,\n",
        "                }\n",
        "\n",
        "                with open(json_path, \"w\") as f:\n",
        "                    json.dump(json_data, f, indent=2)\n",
        "\n",
        "                successful += 1\n",
        "\n",
        "            except Exception:\n",
        "                error += 1\n",
        "                # Uncomment to see errors:\n",
        "                # print(f\"  ‚ùå Error processing {img_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Update stats\n",
        "        total_stats[\"total_images\"] += len(image_files)\n",
        "        total_stats[\"successful\"] += successful\n",
        "        total_stats[\"no_hand\"] += no_hand\n",
        "        total_stats[\"error\"] += error\n",
        "\n",
        "        gesture_stats[gesture_name] = {\n",
        "            \"total\": len(image_files),\n",
        "            \"successful\": successful,\n",
        "            \"no_hand\": no_hand,\n",
        "            \"error\": error,\n",
        "            \"success_rate\": (successful / len(image_files) * 100)\n",
        "            if len(image_files) > 0\n",
        "            else 0,\n",
        "        }\n",
        "\n",
        "        # Print summary for this gesture\n",
        "        print(f\"  ‚úÖ Successful: {successful}\")\n",
        "        print(f\"  ‚ùå No hand detected: {no_hand}\")\n",
        "        print(f\"  ‚ö†Ô∏è  Errors: {error}\")\n",
        "        print(f\"  üìä Success rate: {gesture_stats[gesture_name]['success_rate']:.1f}%\")\n",
        "\n",
        "    hands.close()\n",
        "\n",
        "    # Overall summary\n",
        "    print(f\"\\n\\n{'=' * 60}\")\n",
        "    print(\"‚úÖ PROCESSING COMPLETE\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    print(\"\\nüìä Overall Statistics:\")\n",
        "    print(f\"  Total images processed: {total_stats['total_images']}\")\n",
        "    print(f\"  ‚úÖ Successful: {total_stats['successful']}\")\n",
        "    print(f\"  ‚ùå No hand detected: {total_stats['no_hand']}\")\n",
        "    print(f\"  ‚ö†Ô∏è  Errors: {total_stats['error']}\")\n",
        "\n",
        "    if total_stats[\"total_images\"] > 0:\n",
        "        success_rate = total_stats[\"successful\"] / total_stats[\"total_images\"] * 100\n",
        "        print(f\"  üìà Overall success rate: {success_rate:.1f}%\")\n",
        "\n",
        "    print(\"\\nüì¶ Per-Gesture Statistics:\")\n",
        "    for gesture_name, stats in gesture_stats.items():\n",
        "        status = \"‚úÖ\" if stats[\"success_rate\"] > 80 else \"‚ö†Ô∏è\" if stats[\"success_rate\"] > 50 else \"‚ùå\"\n",
        "        print(\n",
        "            f\"  {status} {gesture_name:12s}: \"\n",
        "            f\"{stats['successful']:4d}/{stats['total']:4d} \"\n",
        "            f\"({stats['success_rate']:.1f}%)\"\n",
        "        )\n",
        "\n",
        "    print(f\"\\nüíæ JSON files saved to: {MANUAL_JSON_DIR}\")\n",
        "    print(f\"   Total JSON files created: {total_stats['successful']}\")\n",
        "\n",
        "    # Tips for improving success rate\n",
        "    if total_stats[\"no_hand\"] > 0:\n",
        "        print(\"\\nüí° Tips to improve success rate:\")\n",
        "        print(f\"   ‚Ä¢ {total_stats['no_hand']} images had no hand detected\")\n",
        "        print(\"   ‚Ä¢ Make sure hand is clearly visible and well-lit\")\n",
        "        print(\"   ‚Ä¢ Avoid blurry or dark images\")\n",
        "        print(\"   ‚Ä¢ Show the full hand gesture in frame\")\n",
        "\n",
        "    return gesture_stats\n",
        "\n",
        "\n",
        "# Run processing\n",
        "print(\"\\nüöÄ Starting automatic processing...\")\n",
        "process_manual_images()"
      ],
      "metadata": {
        "id": "QulofZdvJ00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d9699c-8429-45c0-ac13-b6eaebdea60e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîÑ PROCESSING MANUAL IMAGES FROM FOLDERS\n",
            "============================================================\n",
            "\n",
            "Input: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_manual\n",
            "Output: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_manual_json\n",
            "\n",
            "Expected folder structure:\n",
            "  /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_manual/\n",
            "    ‚îú‚îÄ‚îÄ call/       (image files)\n",
            "    ‚îú‚îÄ‚îÄ like/       (image files)\n",
            "    ‚îú‚îÄ‚îÄ dislike/    (image files)\n",
            "    ‚îú‚îÄ‚îÄ fist/       (image files)\n",
            "    ‚îú‚îÄ‚îÄ palm/       (image files)\n",
            "    ‚îú‚îÄ‚îÄ peace/      (image files)\n",
            "    ‚îî‚îÄ‚îÄ rock/       (image files)\n",
            "\n",
            "üöÄ Starting automatic processing...\n",
            "\n",
            "üìÅ Found 7 gesture folders: ['call', 'dislike', 'fist', 'palm', 'peace', 'rock', 'like']\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: CALL\n",
            "============================================================\n",
            "Found 284 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  Processing call:   0%|          | 0/284 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "  Processing call: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 284/284 [01:29<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 281\n",
            "  ‚ùå No hand detected: 3\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 98.9%\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: DISLIKE\n",
            "============================================================\n",
            "Found 183 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Processing dislike: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 183/183 [00:33<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 183\n",
            "  ‚ùå No hand detected: 0\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 100.0%\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: FIST\n",
            "============================================================\n",
            "Found 191 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Processing fist: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:34<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 188\n",
            "  ‚ùå No hand detected: 3\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 98.4%\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: PALM\n",
            "============================================================\n",
            "Found 150 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Processing palm: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:26<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 150\n",
            "  ‚ùå No hand detected: 0\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 100.0%\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: PEACE\n",
            "============================================================\n",
            "Found 233 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Processing peace: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233/233 [00:40<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 222\n",
            "  ‚ùå No hand detected: 11\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 95.3%\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: ROCK\n",
            "============================================================\n",
            "Found 440 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Processing rock: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440/440 [01:24<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 428\n",
            "  ‚ùå No hand detected: 12\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 97.3%\n",
            "\n",
            "============================================================\n",
            "üì∏ Processing: LIKE\n",
            "============================================================\n",
            "Found 255 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Processing like: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255/255 [00:47<00:00,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Successful: 252\n",
            "  ‚ùå No hand detected: 3\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìä Success rate: 98.8%\n",
            "\n",
            "\n",
            "============================================================\n",
            "‚úÖ PROCESSING COMPLETE\n",
            "============================================================\n",
            "\n",
            "üìä Overall Statistics:\n",
            "  Total images processed: 1736\n",
            "  ‚úÖ Successful: 1704\n",
            "  ‚ùå No hand detected: 32\n",
            "  ‚ö†Ô∏è  Errors: 0\n",
            "  üìà Overall success rate: 98.2%\n",
            "\n",
            "üì¶ Per-Gesture Statistics:\n",
            "  ‚úÖ call        :  281/ 284 (98.9%)\n",
            "  ‚úÖ dislike     :  183/ 183 (100.0%)\n",
            "  ‚úÖ fist        :  188/ 191 (98.4%)\n",
            "  ‚úÖ palm        :  150/ 150 (100.0%)\n",
            "  ‚úÖ peace       :  222/ 233 (95.3%)\n",
            "  ‚úÖ rock        :  428/ 440 (97.3%)\n",
            "  ‚úÖ like        :  252/ 255 (98.8%)\n",
            "\n",
            "üíæ JSON files saved to: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_manual_json\n",
            "   Total JSON files created: 1704\n",
            "\n",
            "üí° Tips to improve success rate:\n",
            "   ‚Ä¢ 32 images had no hand detected\n",
            "   ‚Ä¢ Make sure hand is clearly visible and well-lit\n",
            "   ‚Ä¢ Avoid blurry or dark images\n",
            "   ‚Ä¢ Show the full hand gesture in frame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'call': {'total': 284,\n",
              "  'successful': 281,\n",
              "  'no_hand': 3,\n",
              "  'error': 0,\n",
              "  'success_rate': 98.94366197183099},\n",
              " 'dislike': {'total': 183,\n",
              "  'successful': 183,\n",
              "  'no_hand': 0,\n",
              "  'error': 0,\n",
              "  'success_rate': 100.0},\n",
              " 'fist': {'total': 191,\n",
              "  'successful': 188,\n",
              "  'no_hand': 3,\n",
              "  'error': 0,\n",
              "  'success_rate': 98.42931937172776},\n",
              " 'palm': {'total': 150,\n",
              "  'successful': 150,\n",
              "  'no_hand': 0,\n",
              "  'error': 0,\n",
              "  'success_rate': 100.0},\n",
              " 'peace': {'total': 233,\n",
              "  'successful': 222,\n",
              "  'no_hand': 11,\n",
              "  'error': 0,\n",
              "  'success_rate': 95.27896995708154},\n",
              " 'rock': {'total': 440,\n",
              "  'successful': 428,\n",
              "  'no_hand': 12,\n",
              "  'error': 0,\n",
              "  'success_rate': 97.27272727272728},\n",
              " 'like': {'total': 255,\n",
              "  'successful': 252,\n",
              "  'no_hand': 3,\n",
              "  'error': 0,\n",
              "  'success_rate': 98.82352941176471}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 7: Extract MediaPipe Landmarks with Quality Filtering\n",
        "# ============================================================\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from datetime import datetime\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Skip if not using public data\n",
        "if not USE_PUBLIC_DATA:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚è≠Ô∏è  SKIPPING HAGRID LANDMARK EXTRACTION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"USE_PUBLIC_DATA = False, skipping HaGRID processing\")\n",
        "    print(\"\\n‚úì Moving to next step...\")\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üîß PROCESSING HAGRID WITH MEDIAPIPE + QUALITY FILTERING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    HAGRID_IMAGES_ROOT = HAGRID_SUBSET_DIR\n",
        "\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(\n",
        "        static_image_mode=False,\n",
        "        max_num_hands=1,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "\n",
        "    HAND_JSON_DIR = os.path.join(PROJECT_ROOT, \"data/hand_gestures_public\")\n",
        "    os.makedirs(HAND_JSON_DIR, exist_ok=True)\n",
        "\n",
        "    stats = {\n",
        "        g: {\n",
        "            \"processed\": 0,\n",
        "            \"failed_no_hand\": 0,\n",
        "            \"filtered_too_small\": 0,\n",
        "            \"filtered_too_large\": 0,\n",
        "            \"filtered_off_center\": 0,\n",
        "            \"filtered_low_confidence\": 0\n",
        "        }\n",
        "        for g in HAND_GESTURES.keys()\n",
        "    }\n",
        "\n",
        "    for gesture in HAND_GESTURES.keys():\n",
        "        gesture_dir = os.path.join(HAGRID_IMAGES_ROOT, gesture)\n",
        "\n",
        "        if not os.path.isdir(gesture_dir):\n",
        "            print(f\"‚ö†Ô∏è Skipping '{gesture}': folder not found at {gesture_dir}\")\n",
        "            continue\n",
        "\n",
        "        image_paths = sorted(glob.glob(os.path.join(gesture_dir, \"*.jpeg\")))\n",
        "\n",
        "        if not image_paths:\n",
        "            print(f\"‚ö†Ô∏è No images found in {gesture_dir}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{gesture}: processing {len(image_paths)} images with quality filtering\")\n",
        "\n",
        "        for img_path in tqdm(image_paths, desc=f\"MediaPipe {gesture}\"):\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                stats[gesture][\"failed_no_hand\"] += 1\n",
        "                continue\n",
        "\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(img_rgb)\n",
        "\n",
        "            if not results.multi_hand_landmarks:\n",
        "                stats[gesture][\"failed_no_hand\"] += 1\n",
        "                continue\n",
        "\n",
        "            lm = results.multi_hand_landmarks[0]\n",
        "\n",
        "            # NEW: Quality filtering\n",
        "            # Check 1: Hand size (not too small or large)\n",
        "            x_coords = [p.x for p in lm.landmark]\n",
        "            y_coords = [p.y for p in lm.landmark]\n",
        "            hand_width = max(x_coords) - min(x_coords)\n",
        "            hand_height = max(y_coords) - min(y_coords)\n",
        "\n",
        "            if hand_width < 0.15 or hand_height < 0.15:\n",
        "                stats[gesture][\"filtered_too_small\"] += 1\n",
        "                continue\n",
        "\n",
        "            if hand_width > 0.85 or hand_height > 0.85:\n",
        "                stats[gesture][\"filtered_too_large\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Check 2: Hand position (reasonably centered)\n",
        "            hand_center_x = (max(x_coords) + min(x_coords)) / 2\n",
        "            hand_center_y = (max(y_coords) + min(y_coords)) / 2\n",
        "\n",
        "            if hand_center_x < 0.15 or hand_center_x > 0.85:\n",
        "                stats[gesture][\"filtered_off_center\"] += 1\n",
        "                continue\n",
        "\n",
        "            if hand_center_y < 0.15 or hand_center_y > 0.85:\n",
        "                stats[gesture][\"filtered_off_center\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Check 3: Landmark confidence (MediaPipe provides this)\n",
        "            # Average z-coordinate shouldn't be too far (indicates poor tracking)\n",
        "            z_coords = [p.z for p in lm.landmark]\n",
        "            avg_z = sum(z_coords) / len(z_coords)\n",
        "\n",
        "            if abs(avg_z) > 0.3:  # Too far forward/backward\n",
        "                stats[gesture][\"filtered_low_confidence\"] += 1\n",
        "                continue\n",
        "\n",
        "            # Passed all quality checks - extract landmarks\n",
        "            flat = []\n",
        "            for p in lm.landmark:\n",
        "                flat.extend([p.x, p.y, p.z])\n",
        "\n",
        "            data = {\n",
        "                \"gesture\": gesture,\n",
        "                \"landmarks\": flat,\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "                \"source\": \"hagrid\",\n",
        "                \"image_file\": os.path.basename(img_path),\n",
        "                \"quality\": {\n",
        "                    \"hand_width\": float(hand_width),\n",
        "                    \"hand_height\": float(hand_height),\n",
        "                    \"center_x\": float(hand_center_x),\n",
        "                    \"center_y\": float(hand_center_y),\n",
        "                    \"avg_z\": float(avg_z)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            out_name = f\"{gesture}_{os.path.splitext(os.path.basename(img_path))[0]}.json\"\n",
        "            out_path = os.path.join(HAND_JSON_DIR, out_name)\n",
        "\n",
        "            with open(out_path, \"w\") as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "\n",
        "            stats[gesture][\"processed\"] += 1\n",
        "\n",
        "    hands.close()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ HAND GESTURE PROCESSING SUMMARY (WITH QUALITY FILTERING)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for g, c in stats.items():\n",
        "        total_attempted = (c[\"processed\"] + c[\"failed_no_hand\"] + c[\"filtered_too_small\"] +\n",
        "                          c[\"filtered_too_large\"] + c[\"filtered_off_center\"] + c[\"filtered_low_confidence\"])\n",
        "        if total_attempted == 0:\n",
        "            continue\n",
        "\n",
        "        success_rate = 100 * c[\"processed\"] / total_attempted if total_attempted > 0 else 0\n",
        "\n",
        "        print(f\"\\n{g:15s}:\")\n",
        "        print(f\"  ‚úÖ Accepted: {c['processed']:4d} ({success_rate:.1f}%)\")\n",
        "        print(f\"  ‚ùå No hand:  {c['failed_no_hand']:4d}\")\n",
        "        print(f\"  üîç Filters:\")\n",
        "        print(f\"     Too small:    {c['filtered_too_small']:4d}\")\n",
        "        print(f\"     Too large:    {c['filtered_too_large']:4d}\")\n",
        "        print(f\"     Off-center:   {c['filtered_off_center']:4d}\")\n",
        "        print(f\"     Low conf:     {c['filtered_low_confidence']:4d}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"JSON output dir: {HAND_JSON_DIR}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "Ad561JVn2zlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735d472c-5e08-4bac-c6b3-f720d1837cf5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üîß PROCESSING HAGRID WITH MEDIAPIPE + QUALITY FILTERING\n",
            "============================================================\n",
            "\n",
            "call: processing 1500 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe call: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [06:22<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "like: processing 1500 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe like: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [05:57<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dislike: processing 1500 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe dislike: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [05:38<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "fist: processing 1500 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe fist: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [05:08<00:00,  4.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "palm: processing 1500 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe palm: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [07:22<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "peace: processing 1500 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe peace: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [06:08<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "rock: processing 1584 images with quality filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MediaPipe rock: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1584/1584 [06:52<00:00,  3.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "‚úÖ HAND GESTURE PROCESSING SUMMARY (WITH QUALITY FILTERING)\n",
            "============================================================\n",
            "\n",
            "call           :\n",
            "  ‚úÖ Accepted:  861 (57.4%)\n",
            "  ‚ùå No hand:   594\n",
            "  üîç Filters:\n",
            "     Too small:      37\n",
            "     Too large:       6\n",
            "     Off-center:      0\n",
            "     Low conf:        2\n",
            "\n",
            "like           :\n",
            "  ‚úÖ Accepted:  837 (55.8%)\n",
            "  ‚ùå No hand:   562\n",
            "  üîç Filters:\n",
            "     Too small:      94\n",
            "     Too large:       4\n",
            "     Off-center:      0\n",
            "     Low conf:        3\n",
            "\n",
            "dislike        :\n",
            "  ‚úÖ Accepted:  803 (53.5%)\n",
            "  ‚ùå No hand:   537\n",
            "  üîç Filters:\n",
            "     Too small:     154\n",
            "     Too large:       1\n",
            "     Off-center:      0\n",
            "     Low conf:        5\n",
            "\n",
            "fist           :\n",
            "  ‚úÖ Accepted:  728 (48.5%)\n",
            "  ‚ùå No hand:   516\n",
            "  üîç Filters:\n",
            "     Too small:     254\n",
            "     Too large:       0\n",
            "     Off-center:      0\n",
            "     Low conf:        2\n",
            "\n",
            "palm           :\n",
            "  ‚úÖ Accepted: 1086 (72.4%)\n",
            "  ‚ùå No hand:   358\n",
            "  üîç Filters:\n",
            "     Too small:      28\n",
            "     Too large:      27\n",
            "     Off-center:      0\n",
            "     Low conf:        1\n",
            "\n",
            "peace          :\n",
            "  ‚úÖ Accepted:  842 (56.1%)\n",
            "  ‚ùå No hand:   416\n",
            "  üîç Filters:\n",
            "     Too small:     202\n",
            "     Too large:      30\n",
            "     Off-center:      0\n",
            "     Low conf:       10\n",
            "\n",
            "rock           :\n",
            "  ‚úÖ Accepted: 1010 (63.8%)\n",
            "  ‚ùå No hand:   417\n",
            "  üîç Filters:\n",
            "     Too small:     143\n",
            "     Too large:       8\n",
            "     Off-center:      0\n",
            "     Low conf:        6\n",
            "\n",
            "============================================================\n",
            "JSON output dir: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_public\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8: Data Preprocessing (with Manual + Public Support)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "def simple_train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
        "    \"\"\"Stratified split ensures each class is proportionally represented.\"\"\"\n",
        "    assert len(X) == len(y)\n",
        "\n",
        "    # First split: train vs (val+test)\n",
        "    sss1 = StratifiedShuffleSplit(n_splits=1, train_size=train_ratio, random_state=seed)\n",
        "    train_idx, temp_idx = next(sss1.split(X, y))\n",
        "\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_temp, y_temp = X[temp_idx], y[temp_idx]\n",
        "\n",
        "    # Second split: val vs test\n",
        "    val_ratio_adjusted = val_ratio / (val_ratio + (1 - train_ratio - val_ratio))\n",
        "    sss2 = StratifiedShuffleSplit(\n",
        "        n_splits=1, train_size=val_ratio_adjusted, random_state=seed\n",
        "    )\n",
        "    val_idx, test_idx = next(sss2.split(X_temp, y_temp))\n",
        "\n",
        "    X_val, y_val = X_temp[val_idx], y_temp[val_idx]\n",
        "    X_test, y_test = X_temp[test_idx], y_temp[test_idx]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "class DatasetBuilder:\n",
        "    def __init__(self, json_dirs, gesture_type):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            json_dirs: list of directories containing JSON files\n",
        "            gesture_type: \"hand\"\n",
        "        \"\"\"\n",
        "        self.json_dirs = json_dirs if isinstance(json_dirs, list) else [json_dirs]\n",
        "        self.gesture_type = gesture_type\n",
        "        self.class_names = []\n",
        "        self.feature_mean = None\n",
        "        self.feature_std = None\n",
        "\n",
        "    def _load_single_file(self, path):\n",
        "        with open(path, \"r\") as f:\n",
        "            sample = json.load(f)\n",
        "\n",
        "        gesture = sample.get(\"gesture\")\n",
        "        landmarks = sample.get(\"landmarks\")\n",
        "\n",
        "        # Filter out unwanted gestures for hand dataset\n",
        "        if self.gesture_type == \"hand\" and gesture not in ALLOWED_HAND_GESTURES:\n",
        "            return None, None\n",
        "\n",
        "        # landmarks must be a flat list of numbers\n",
        "        if isinstance(landmarks, dict):\n",
        "            return None, None\n",
        "\n",
        "        if not isinstance(landmarks, (list, tuple)):\n",
        "            return None, None\n",
        "\n",
        "        # Flatten nested lists if any\n",
        "        flat = []\n",
        "        for v in landmarks:\n",
        "            if isinstance(v, (list, tuple)):\n",
        "                flat.extend(v)\n",
        "            else:\n",
        "                flat.append(v)\n",
        "\n",
        "        # Ensure all are numeric\n",
        "        try:\n",
        "            flat = [float(x) for x in flat]\n",
        "        except Exception:\n",
        "            return None, None\n",
        "\n",
        "        return gesture, flat\n",
        "\n",
        "    def load_jsons(self):\n",
        "        \"\"\"Load JSONs from all configured directories.\"\"\"\n",
        "        data = []\n",
        "        labels = []\n",
        "\n",
        "        for json_dir in self.json_dirs:\n",
        "            if not os.path.isdir(json_dir):\n",
        "                print(f\"‚ö†Ô∏è JSON dir not found: {json_dir}, skipping\")\n",
        "                continue\n",
        "\n",
        "            files = [f for f in os.listdir(json_dir) if f.endswith(\".json\")]\n",
        "            print(f\"Found {len(files)} JSON files in {json_dir}\")\n",
        "\n",
        "            skipped = 0\n",
        "            before = len(data)\n",
        "\n",
        "            for fname in tqdm(files, desc=f\"Loading from {os.path.basename(json_dir)}\"):\n",
        "                path = os.path.join(json_dir, fname)\n",
        "                gesture, flat = self._load_single_file(path)\n",
        "                if gesture is None or flat is None:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "\n",
        "                data.append(flat)\n",
        "                labels.append(gesture)\n",
        "\n",
        "            loaded_here = len(data) - before\n",
        "            print(\n",
        "                f\"Loaded {loaded_here} samples from this directory, \"\n",
        "                f\"skipped {skipped} bad files\"\n",
        "            )\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(f\"‚ö†Ô∏è No valid data loaded for {self.gesture_type}\")\n",
        "            return np.empty((0,)), np.empty((0,))\n",
        "\n",
        "        X = np.array(data, dtype=np.float32)\n",
        "        y = np.array(labels, dtype=object)\n",
        "        return X, y\n",
        "\n",
        "    def augment_landmarks(self, X, y, augment_factor=0):\n",
        "        \"\"\"Create augmented copies with varied perturbations.\"\"\"\n",
        "        aug_X = [X]  # Original data\n",
        "        aug_y = [y]\n",
        "\n",
        "        for _ in range(augment_factor):\n",
        "            # Stronger random translation\n",
        "            shift_x = np.random.uniform(-0.05, 0.05, size=(len(X), 21))\n",
        "            shift_y = np.random.uniform(-0.05, 0.05, size=(len(X), 21))\n",
        "            shift_z = np.random.uniform(-0.02, 0.02, size=(len(X), 21))\n",
        "\n",
        "            X_shifted = X.copy().reshape(len(X), 21, 3)\n",
        "            X_shifted[:, :, 0] += shift_x  # x coords\n",
        "            X_shifted[:, :, 1] += shift_y  # y coords\n",
        "            X_shifted[:, :, 2] += shift_z  # z coords\n",
        "            X_shifted = X_shifted.reshape(len(X), 63)\n",
        "\n",
        "            # Stronger Gaussian noise\n",
        "            X_noisy = X_shifted + np.random.normal(0, 0.01, X_shifted.shape)\n",
        "\n",
        "            # Random scaling\n",
        "            scale = np.random.uniform(0.9, 1.1, size=(len(X), 1))\n",
        "            X_scaled = X_noisy * scale\n",
        "\n",
        "            # Random rotation around z-axis\n",
        "            angle = np.random.uniform(-0.2, 0.2, size=(len(X),))\n",
        "            cos_a = np.cos(angle)[:, None]\n",
        "            sin_a = np.sin(angle)[:, None]\n",
        "            X_rot = X_scaled.copy().reshape(len(X), 21, 3)\n",
        "            x_new = X_rot[:, :, 0] * cos_a - X_rot[:, :, 1] * sin_a\n",
        "            y_new = X_rot[:, :, 0] * sin_a + X_rot[:, :, 1] * cos_a\n",
        "            X_rot[:, :, 0] = x_new\n",
        "            X_rot[:, :, 1] = y_new\n",
        "            X_rot = X_rot.reshape(len(X), 63)\n",
        "\n",
        "            aug_X.append(X_rot)\n",
        "            aug_y.append(y.copy())\n",
        "\n",
        "        return np.vstack(aug_X), np.hstack(aug_y)\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        X, y = self.load_jsons()\n",
        "        if X.size == 0:\n",
        "            raise RuntimeError(f\"No data loaded for gesture_type={self.gesture_type}\")\n",
        "\n",
        "        print(f\"\\nüìä Total raw samples loaded: {len(X)}\")\n",
        "\n",
        "        # Augment before normalization\n",
        "        X_aug, y_aug = self.augment_landmarks(X, y, augment_factor=0)\n",
        "        print(f\"üìä After augmentation: {len(X_aug)} samples\")\n",
        "\n",
        "        # Feature normalization\n",
        "        self.feature_mean = X_aug.mean(axis=0, keepdims=True)\n",
        "        self.feature_std = X_aug.std(axis=0, keepdims=True) + 1e-6\n",
        "        X_norm = (X_aug - self.feature_mean) / self.feature_std\n",
        "\n",
        "        # Class encoding\n",
        "        self.class_names = sorted(list(set(y_aug)))\n",
        "        print(f\"\\nClasses ({self.gesture_type}): {self.class_names}\")\n",
        "\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(self.class_names)}\n",
        "        y_encoded = np.array([label_to_idx[label] for label in y_aug], dtype=np.int64)\n",
        "\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = simple_train_val_test_split(\n",
        "            X_norm, y_encoded, train_ratio=0.7, val_ratio=0.15, seed=42\n",
        "        )\n",
        "\n",
        "        dataset = {\n",
        "            \"train_data\": X_train,\n",
        "            \"train_labels\": y_train,\n",
        "            \"val_data\": X_val,\n",
        "            \"val_labels\": y_val,\n",
        "            \"test_data\": X_test,\n",
        "            \"test_labels\": y_test,\n",
        "            \"class_names\": self.class_names,\n",
        "            \"feature_mean\": self.feature_mean.astype(np.float32),\n",
        "            \"feature_std\": self.feature_std.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        print(\n",
        "            f\"\\n{self.gesture_type.upper()} dataset sizes ‚Üí \"\n",
        "            f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\"\n",
        "        )\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Build dataset based on configuration flags\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BUILDING HAND GESTURE DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Determine which directories to include\n",
        "json_dirs_to_use = []\n",
        "\n",
        "if USE_PUBLIC_DATA:\n",
        "    public_dir = os.path.join(PROJECT_ROOT, \"data/hand_gestures_public\")\n",
        "    if os.path.exists(public_dir):\n",
        "        json_dirs_to_use.append(public_dir)\n",
        "        print(f\"‚úì Including public HaGRID data from: {public_dir}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Public data directory not found: {public_dir}\")\n",
        "\n",
        "if USE_MANUAL_DATA:\n",
        "    manual_dir = os.path.join(PROJECT_ROOT, \"data/hand_gestures_manual_json\")\n",
        "    if os.path.exists(manual_dir):\n",
        "        json_dirs_to_use.append(manual_dir)\n",
        "        print(f\"‚úì Including manual collected data from: {manual_dir}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Manual data directory not found: {manual_dir}\")\n",
        "\n",
        "if len(json_dirs_to_use) == 0:\n",
        "    raise RuntimeError(\n",
        "        \"‚ùå No data sources enabled! Set USE_PUBLIC_DATA or USE_MANUAL_DATA to True\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nüìÇ Total data sources: {len(json_dirs_to_use)}\")\n",
        "\n",
        "# Build combined dataset\n",
        "hand_builder = DatasetBuilder(json_dirs_to_use, \"hand\")\n",
        "hand_dataset = hand_builder.prepare_dataset()\n",
        "\n",
        "# Save dataset\n",
        "os.makedirs(os.path.join(PROJECT_ROOT, \"data\"), exist_ok=True)\n",
        "with open(os.path.join(PROJECT_ROOT, \"data/hand_gesture_dataset.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(hand_dataset, f)\n",
        "\n",
        "print(\"\\n‚úì Hand dataset saved\")"
      ],
      "metadata": {
        "id": "9YfuqDK722Xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63677d1a-0d45-4dce-bfd9-ee1d3ba84266"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BUILDING HAND GESTURE DATASET\n",
            "============================================================\n",
            "‚úì Including public HaGRID data from: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_public\n",
            "‚úì Including manual collected data from: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_manual_json\n",
            "\n",
            "üìÇ Total data sources: 2\n",
            "Found 9109 JSON files in /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_public\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading from hand_gestures_public: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9109/9109 [01:54<00:00, 79.55it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8691 samples from this directory, skipped 418 bad files\n",
            "Found 15927 JSON files in /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/data/hand_gestures_manual_json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading from hand_gestures_manual_json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15927/15927 [06:20<00:00, 41.90it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 15927 samples from this directory, skipped 0 bad files\n",
            "\n",
            "üìä Total raw samples loaded: 24618\n",
            "üìä After augmentation: 24618 samples\n",
            "\n",
            "Classes (hand): ['call', 'dislike', 'fist', 'like', 'palm', 'peace', 'rock']\n",
            "\n",
            "HAND dataset sizes ‚Üí Train: 17232, Val: 3692, Test: 3694\n",
            "\n",
            "‚úì Hand dataset saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# üîß CELL 9: MODEL TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "\n",
        "class GestureModelTrainer:\n",
        "    \"\"\"Unified trainer for gesture recognition models.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, model_name, strategy):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset: dict with train/val/test data and labels\n",
        "            model_name: str, name for saving\n",
        "            strategy: tf.distribute.Strategy (from CELL 1B)\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.model_name = model_name\n",
        "        self.strategy = strategy\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "        # Extract dataset info\n",
        "        self.class_names = dataset[\"class_names\"]\n",
        "        self.num_classes = len(self.class_names)\n",
        "\n",
        "        # Determine input size from data shape\n",
        "        self.input_size = dataset[\"train_data\"].shape[1]\n",
        "\n",
        "        print(f\"Initialized trainer for '{model_name}'\")\n",
        "        print(f\"  Input size: {self.input_size}\")\n",
        "        print(f\"  Classes: {self.num_classes} ‚Üí {self.class_names}\")\n",
        "        print(f\"  Train samples: {len(dataset['train_data'])}\")\n",
        "        print(f\"  Val samples: {len(dataset['val_data'])}\")\n",
        "        print(f\"  Test samples: {len(dataset['test_data'])}\")\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build improved neural network with better architecture.\"\"\"\n",
        "        with self.strategy.scope():\n",
        "            model = keras.Sequential(\n",
        "                [\n",
        "                    # Input layer\n",
        "                    layers.Input(shape=(self.input_size,)),\n",
        "\n",
        "                    # Layer 1: Feature extraction\n",
        "                    layers.Dense(\n",
        "                        256,\n",
        "                        activation=\"relu\",\n",
        "                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                    ),\n",
        "                    layers.BatchNormalization(),\n",
        "                    layers.Dropout(0.3),\n",
        "\n",
        "                    # Layer 2: Deep feature learning\n",
        "                    layers.Dense(\n",
        "                        128,\n",
        "                        activation=\"relu\",\n",
        "                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                    ),\n",
        "                    layers.BatchNormalization(),\n",
        "                    layers.Dropout(0.3),\n",
        "\n",
        "                    # Layer 3: Refined features\n",
        "                    layers.Dense(\n",
        "                        64,\n",
        "                        activation=\"relu\",\n",
        "                        kernel_regularizer=regularizers.l2(0.001),\n",
        "                    ),\n",
        "                    layers.BatchNormalization(),\n",
        "                    layers.Dropout(0.2),\n",
        "\n",
        "                    # Output layer\n",
        "                    layers.Dense(self.num_classes, activation=\"softmax\"),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Compile with improved settings\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"],\n",
        "            )\n",
        "\n",
        "        self.model = model\n",
        "        print(\"\\nModel architecture built:\")\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=100, batch_size=256):\n",
        "        \"\"\"Train the model with callbacks.\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # Prepare data\n",
        "        X_train = self.dataset[\"train_data\"]\n",
        "        y_train = self.dataset[\"train_labels\"]\n",
        "        X_val = self.dataset[\"val_data\"]\n",
        "        y_val = self.dataset[\"val_labels\"]\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_accuracy\",\n",
        "                patience=15,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "            ),\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_loss\",\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"TRAINING {self.model_name.upper()}\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(f\"Epochs: {epochs}\")\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        print(f\"Training on {len(X_train)} samples\")\n",
        "        print(f\"Validating on {len(X_val)} samples\")\n",
        "        print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "        # Train\n",
        "        self.history = self.model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "        # Evaluate on test set\n",
        "        X_test = self.dataset[\"test_data\"]\n",
        "        y_test = self.dataset[\"test_labels\"]\n",
        "\n",
        "        test_loss, test_acc = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(\"TRAINING COMPLETE\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(f\"Final validation accuracy: {self.history.history['val_accuracy'][-1]:.4f}\")\n",
        "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "        print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAIN THE MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CREATING AND TRAINING HAND GESTURE MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create trainer\n",
        "hand_trainer = GestureModelTrainer(\n",
        "    dataset=hand_dataset,\n",
        "    model_name=\"hand_gesture\",\n",
        "    strategy=strategy,  # from CELL 1B\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = hand_trainer.train(\n",
        "    epochs=100,\n",
        "    batch_size=256,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Model training complete!\")\n",
        "print(\"   hand_trainer object created and ready\")\n",
        "print(\"   hand_trainer.model is the trained Keras model\")"
      ],
      "metadata": {
        "id": "z07aj7auUbH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7080f761-27ea-4c70-daaa-d3ff849472b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CREATING AND TRAINING HAND GESTURE MODEL\n",
            "============================================================\n",
            "Initialized trainer for 'hand_gesture'\n",
            "  Input size: 63\n",
            "  Classes: 7 ‚Üí ['call', 'dislike', 'fist', 'like', 'palm', 'peace', 'rock']\n",
            "  Train samples: 17232\n",
            "  Val samples: 3692\n",
            "  Test samples: 3694\n",
            "\n",
            "Model architecture built:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            ‚îÇ        \u001b[38;5;34m16,384\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            ‚îÇ         \u001b[38;5;34m1,024\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m32,896\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_1           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ           \u001b[38;5;34m512\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ         \u001b[38;5;34m8,256\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_2           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ           \u001b[38;5;34m256\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              ‚îÇ           \u001b[38;5;34m455\u001b[0m ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_1           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_2           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,783\u001b[0m (233.53 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,783</span> (233.53 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,887\u001b[0m (230.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,887</span> (230.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING HAND_GESTURE\n",
            "============================================================\n",
            "Epochs: 100\n",
            "Batch size: 256\n",
            "Training on 17232 samples\n",
            "Validating on 3692 samples\n",
            "============================================================\n",
            "\n",
            "Epoch 1/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5730 - loss: 1.6321 - val_accuracy: 0.9596 - val_loss: 1.0271 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9542 - loss: 0.5317 - val_accuracy: 0.9756 - val_loss: 0.6035 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9732 - loss: 0.4365 - val_accuracy: 0.9837 - val_loss: 0.4391 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9768 - loss: 0.3917 - val_accuracy: 0.9875 - val_loss: 0.3575 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9825 - loss: 0.3524 - val_accuracy: 0.9889 - val_loss: 0.3205 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9826 - loss: 0.3260 - val_accuracy: 0.9878 - val_loss: 0.2936 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9821 - loss: 0.3017 - val_accuracy: 0.9875 - val_loss: 0.2704 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9847 - loss: 0.2764 - val_accuracy: 0.9897 - val_loss: 0.2494 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9816 - loss: 0.2638 - val_accuracy: 0.9892 - val_loss: 0.2294 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9856 - loss: 0.2391 - val_accuracy: 0.9886 - val_loss: 0.2144 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9857 - loss: 0.2219 - val_accuracy: 0.9894 - val_loss: 0.1992 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9857 - loss: 0.2005 - val_accuracy: 0.9892 - val_loss: 0.1863 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9843 - loss: 0.1952 - val_accuracy: 0.9889 - val_loss: 0.1745 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9868 - loss: 0.1822 - val_accuracy: 0.9889 - val_loss: 0.1650 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9844 - loss: 0.1731 - val_accuracy: 0.9873 - val_loss: 0.1587 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9859 - loss: 0.1591 - val_accuracy: 0.9884 - val_loss: 0.1471 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9869 - loss: 0.1470 - val_accuracy: 0.9884 - val_loss: 0.1410 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9854 - loss: 0.1441 - val_accuracy: 0.9881 - val_loss: 0.1331 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9871 - loss: 0.1377 - val_accuracy: 0.9892 - val_loss: 0.1265 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9879 - loss: 0.1268 - val_accuracy: 0.9905 - val_loss: 0.1228 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9872 - loss: 0.1227 - val_accuracy: 0.9894 - val_loss: 0.1186 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9877 - loss: 0.1168 - val_accuracy: 0.9897 - val_loss: 0.1136 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9881 - loss: 0.1134 - val_accuracy: 0.9881 - val_loss: 0.1082 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9875 - loss: 0.1084 - val_accuracy: 0.9897 - val_loss: 0.1074 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9886 - loss: 0.1058 - val_accuracy: 0.9897 - val_loss: 0.1065 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9896 - loss: 0.0985 - val_accuracy: 0.9908 - val_loss: 0.0958 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9888 - loss: 0.0956 - val_accuracy: 0.9894 - val_loss: 0.0955 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9878 - loss: 0.1013 - val_accuracy: 0.9886 - val_loss: 0.1009 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9872 - loss: 0.0999 - val_accuracy: 0.9886 - val_loss: 0.0974 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9866 - loss: 0.0977 - val_accuracy: 0.9884 - val_loss: 0.0952 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9887 - loss: 0.0917 - val_accuracy: 0.9889 - val_loss: 0.0912 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9893 - loss: 0.0913 - val_accuracy: 0.9884 - val_loss: 0.0921 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9899 - loss: 0.0877 - val_accuracy: 0.9894 - val_loss: 0.0887 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9885 - loss: 0.0906 - val_accuracy: 0.9897 - val_loss: 0.0895 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9873 - loss: 0.0935 - val_accuracy: 0.9905 - val_loss: 0.0863 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9893 - loss: 0.0857 - val_accuracy: 0.9900 - val_loss: 0.0843 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9889 - loss: 0.0852 - val_accuracy: 0.9900 - val_loss: 0.0845 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9875 - loss: 0.0864 - val_accuracy: 0.9862 - val_loss: 0.0947 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9860 - loss: 0.0926 - val_accuracy: 0.9894 - val_loss: 0.0878 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9891 - loss: 0.0822 - val_accuracy: 0.9902 - val_loss: 0.0861 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m63/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9905 - loss: 0.0784\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9904 - loss: 0.0788 - val_accuracy: 0.9900 - val_loss: 0.0845 - learning_rate: 0.0010\n",
            "Epoch 41: early stopping\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE\n",
            "============================================================\n",
            "Final validation accuracy: 0.9900\n",
            "Test accuracy: 0.9878\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚úÖ Model training complete!\n",
            "   hand_trainer object created and ready\n",
            "   hand_trainer.model is the trained Keras model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# üîß CELL 10: SAVE MODEL (.tflite + .json with mean/std)\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING MODEL (.tflite) AND LABELS (.json)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create models directory\n",
        "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Convert Keras model to TFLite\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüì¶ Converting Keras model to TFLite...\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(hand_trainer.model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_path = os.path.join(MODELS_DIR, \"hand_gesture_model.tflite\")\n",
        "with open(tflite_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "tflite_size_kb = len(tflite_model) / 1024.0\n",
        "\n",
        "print(f\"‚úÖ TFLite model saved to: {tflite_path}\")\n",
        "print(f\"   Model size: {tflite_size_kb:.2f} KB\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Save labels with normalization parameters (CRITICAL FOR ANDROID)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüìã Saving gesture labels with normalization parameters...\")\n",
        "\n",
        "# CRITICAL: HandGestureClassifier.kt expects feature_mean and feature_std!\n",
        "labels = hand_dataset[\"class_names\"]\n",
        "labels_data = {\n",
        "    \"labels\": labels,\n",
        "    \"feature_mean\": hand_dataset[\"feature_mean\"].flatten().tolist(),\n",
        "    \"feature_std\": hand_dataset[\"feature_std\"].flatten().tolist(),\n",
        "}\n",
        "\n",
        "labels_path = os.path.join(MODELS_DIR, \"gesture_labels.json\")\n",
        "with open(labels_path, \"w\") as f:\n",
        "    json.dump(labels_data, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Labels saved to: {labels_path}\")\n",
        "print(f\"   Classes: {labels_data['labels']}\")\n",
        "print(f\"   Feature mean len: {len(labels_data['feature_mean'])} (should be 63)\")\n",
        "print(f\"   Feature std len:  {len(labels_data['feature_std'])} (should be 63)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Android-friendly copies\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüì± Creating Android-compatible file names...\")\n",
        "\n",
        "android_model_path = os.path.join(MODELS_DIR, \"hand_model.tflite\")\n",
        "android_labels_path = os.path.join(MODELS_DIR, \"hand_labels.json\")\n",
        "\n",
        "shutil.copy(tflite_path, android_model_path)\n",
        "shutil.copy(labels_path, android_labels_path)\n",
        "\n",
        "print(f\"‚úÖ Android model:  {android_model_path}\")\n",
        "print(f\"‚úÖ Android labels: {android_labels_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Verification summary\n",
        "# ============================================================================\n",
        "\n",
        "# Evaluate test accuracy\n",
        "test_loss, test_acc = hand_trainer.model.evaluate(\n",
        "    hand_dataset[\"test_data\"],\n",
        "    hand_dataset[\"test_labels\"],\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "total_samples = (\n",
        "    len(hand_dataset[\"train_data\"])\n",
        "    + len(hand_dataset[\"val_data\"])\n",
        "    + len(hand_dataset[\"test_data\"])\n",
        ")\n",
        "\n",
        "dataset_type = []\n",
        "if USE_PUBLIC_DATA:\n",
        "    dataset_type.append(\"Public\")\n",
        "if USE_MANUAL_DATA:\n",
        "    dataset_type.append(\"Manual\")\n",
        "dataset_str = \" + \".join(dataset_type) if dataset_type else \"Unknown\"\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"‚úÖ MODEL EXPORT COMPLETE!\")\n",
        "print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "print(f\"üìÅ Files saved in: {MODELS_DIR}\\n\")\n",
        "print(\"Training Files (descriptive names):\")\n",
        "print(f\"  ‚Ä¢ hand_gesture_model.tflite  ({tflite_size_kb:.2f} KB)\")\n",
        "print(\"  ‚Ä¢ gesture_labels.json\\n\")\n",
        "print(\"Android Files (expected names):\")\n",
        "print(f\"  ‚Ä¢ hand_model.tflite          ({tflite_size_kb:.2f} KB)\")\n",
        "print(\"  ‚Ä¢ hand_labels.json\\n\")\n",
        "\n",
        "print(\"üìä Model Details:\")\n",
        "print(\"  ‚Ä¢ Input shape: (1, 63)  [21 landmarks √ó 3 coordinates]\")\n",
        "print(\n",
        "    f\"  ‚Ä¢ Output shape: (1, {len(labels_data['labels'])})  \"\n",
        "    f\"[{len(labels_data['labels'])} gesture classes]\"\n",
        ")\n",
        "print(f\"  ‚Ä¢ Gesture classes: {', '.join(labels_data['labels'])}\")\n",
        "print(f\"  ‚Ä¢ Trained on: {dataset_str} data\")\n",
        "print(f\"  ‚Ä¢ Total samples: {total_samples}\")\n",
        "print(f\"  ‚Ä¢ Test accuracy: {test_acc:.4f}\\n\")\n",
        "\n",
        "print(\"‚öôÔ∏è  JSON Structure (for Android):\")\n",
        "print(\"{\")\n",
        "print(f\"  \\\"labels\\\": [...],        # List of {len(labels_data['labels'])} gesture names\")\n",
        "print(\"  \\\"feature_mean\\\": [...],  # 63 normalization means\")\n",
        "print(\"  \\\"feature_std\\\": [...]    # 63 normalization stds\")\n",
        "print(\"}\\n\")\n",
        "\n",
        "print(\"üì± Next Steps:\")\n",
        "print(f\"  1. Download both files from: {MODELS_DIR}/\")\n",
        "print(\"  2. Copy to your Android project: /src/main/assets/\")\n",
        "print(\"     ‚Ä¢ hand_model.tflite\")\n",
        "print(\"     ‚Ä¢ hand_labels.json\")\n",
        "print(\"  3. HandGestureClassifier.kt will automatically:\")\n",
        "print(\"     ‚Ä¢ Load the model\")\n",
        "print(\"     ‚Ä¢ Load normalization parameters\")\n",
        "print(\"     ‚Ä¢ Preprocess landmarks with correct mean/std\")\n",
        "print(\"     ‚Ä¢ Predict gestures accurately\\n\")\n",
        "\n",
        "print(\"üîß The preprocessing in Python MATCHES Android exactly:\")\n",
        "print(\"   normalized = (landmarks - feature_mean) / (feature_std + 1e-6)\\n\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2cSiaCnYIVy",
        "outputId": "01e49ad8-000c-4dba-b97b-93a4a03e6b84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SAVING MODEL (.tflite) AND LABELS (.json)\n",
            "============================================================\n",
            "\n",
            "üì¶ Converting Keras model to TFLite...\n",
            "Saved artifact at '/tmp/tmpl_b3gij6'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 63), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132291196125008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291196124816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100444816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100445008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100443472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100443664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291196123664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100445776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100446352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100446544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100444240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100443280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100445584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100445200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100447504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100447696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100446928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100444624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100448272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132291100448848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "‚úÖ TFLite model saved to: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/models/hand_gesture_model.tflite\n",
            "   Model size: 67.32 KB\n",
            "\n",
            "üìã Saving gesture labels with normalization parameters...\n",
            "‚úÖ Labels saved to: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/models/gesture_labels.json\n",
            "   Classes: ['call', 'dislike', 'fist', 'like', 'palm', 'peace', 'rock']\n",
            "   Feature mean len: 63 (should be 63)\n",
            "   Feature std len:  63 (should be 63)\n",
            "\n",
            "üì± Creating Android-compatible file names...\n",
            "‚úÖ Android model:  /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/models/hand_model.tflite\n",
            "‚úÖ Android labels: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/models/hand_labels.json\n",
            "\n",
            "============================================================\n",
            "‚úÖ MODEL EXPORT COMPLETE!\n",
            "============================================================\n",
            "\n",
            "üìÅ Files saved in: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/models\n",
            "\n",
            "Training Files (descriptive names):\n",
            "  ‚Ä¢ hand_gesture_model.tflite  (67.32 KB)\n",
            "  ‚Ä¢ gesture_labels.json\n",
            "\n",
            "Android Files (expected names):\n",
            "  ‚Ä¢ hand_model.tflite          (67.32 KB)\n",
            "  ‚Ä¢ hand_labels.json\n",
            "\n",
            "üìä Model Details:\n",
            "  ‚Ä¢ Input shape: (1, 63)  [21 landmarks √ó 3 coordinates]\n",
            "  ‚Ä¢ Output shape: (1, 7)  [7 gesture classes]\n",
            "  ‚Ä¢ Gesture classes: call, dislike, fist, like, palm, peace, rock\n",
            "  ‚Ä¢ Trained on: Public + Manual data\n",
            "  ‚Ä¢ Total samples: 24618\n",
            "  ‚Ä¢ Test accuracy: 0.9878\n",
            "\n",
            "‚öôÔ∏è  JSON Structure (for Android):\n",
            "{\n",
            "  \"labels\": [...],        # List of 7 gesture names\n",
            "  \"feature_mean\": [...],  # 63 normalization means\n",
            "  \"feature_std\": [...]    # 63 normalization stds\n",
            "}\n",
            "\n",
            "üì± Next Steps:\n",
            "  1. Download both files from: /content/gdrive/MyDrive/Colab_Notebooks/HCI/Project/GestureTouchlessControl/python_training/models/\n",
            "  2. Copy to your Android project: /src/main/assets/\n",
            "     ‚Ä¢ hand_model.tflite\n",
            "     ‚Ä¢ hand_labels.json\n",
            "  3. HandGestureClassifier.kt will automatically:\n",
            "     ‚Ä¢ Load the model\n",
            "     ‚Ä¢ Load normalization parameters\n",
            "     ‚Ä¢ Preprocess landmarks with correct mean/std\n",
            "     ‚Ä¢ Predict gestures accurately\n",
            "\n",
            "üîß The preprocessing in Python MATCHES Android exactly:\n",
            "   normalized = (landmarks - feature_mean) / (feature_std + 1e-6)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ta_GLWOYLmQ"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}